# 随机森林

---

[TOC]

## 1. Bagging

- 思想：总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。
- 举例：假设有1000个样本，如果按照以前的思维，是直接把这1000个样本拿来训练，但现在不一样，先抽取800个样本来进行训练，假如噪声点是这800个样本以外的样本点，就很有效的避开了。重复以上操作，提高模型输出的平均值。

## 2. 随机森林

### 1. 原理

随机森立是 Bagging 的优化版本。其包含的思想在于： **随机选择样本数建立多个训练集并随机选取特征集合，根据多个训练集与特征集合来建立多颗决策树，然后进行投票决策。**

随机森林的最终目的是建立 m 颗决策树，而每颗决策树的建立过程如下：

- 如果训练集大小为N，对于每棵树而言，**随机**且有放回地从训练集中的抽取N个训练样本，作为该树的训练集。
- 如果每个样本的特征维度为M，指定一个常数m<<M，**随机**地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的
- 每棵树都尽最大程度的生长，并且没有剪枝过程。

随机森林中的随性性指的是：**数据采样的随机性与特征采用的随机性。** 这两个随机性的引入对随机森林的分类性能直观重要，它们使得随机森林不容易陷入过拟合，且具有很好的抗噪能力。

### 2. 影响因素

- 森林中任意两棵树的相关性： 相关性越大，错误率越大
- 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。

### 3. 特征 m 的选择

m 是随机森林中唯一的一个参数。

- 减小特征选择个数m，树的相关性和分类能力也会相应的降低
- 增大m，两者也会随之增大。

---

## QA

### 1. 随机森林的过拟合如何解决？

通过交叉验证来调整树的数量。

### 2. 随机怎了如何处理缺失值？

- 首先，给缺失值预设一些估计值， 如平均数，中位数等
- 然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径.
- 判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N
- 如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。

### 3. 什么是 OOB？

OOB 即 out-of-bag ， 又称袋外数据。 这是由于 Bagging 方法会采用 Boostrap 进行抽样， 每次约有 $\frac{1}{3}$ 的样本不会出现在抽样后的样本集合中，那么就把这 $\frac{1}{3}$ 的样本称为袋外数据 oob(out-of-bag)。由于 oob 没有用于训练决策树，因此可用于后续对该决策树的泛化能力评估。

