# Trick - 学习率衰减

tags: 深度学习

---

## 为什么要进行学习率衰减？

随着训练的加深，当模型训练到一定程度后，损失将不再减少，这个时候模型的一阶梯度接近于0， 对应的 Hessian 矩阵通常是两种情况：

- 正定，即所有特征值均为正，此时通常可以得到一个局部极小值，若这个局部极小值接近全局最小则模型已经能得到不错  的性能了，但若差距很大，则模型性能还有待于提升，通常情况下后者在训练初最常见。
- 特征值有正有负，此时模型很可能陷入了鞍点，若陷入鞍点，模型性能表现就很差。

以上两种情况在训练初期以及中期，此时若仍然以固定的学习率，会使模型陷入左右来回的震荡或者鞍点，无法继续优化。所以，学习率衰减或者增大能帮助模型有效的减少震荡或者逃离鞍点。

## 学习率衰减策略

参见 500 questions 第14 章

