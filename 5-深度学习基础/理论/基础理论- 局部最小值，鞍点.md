# 局部最小值，鞍点

---

## 基础原理

[神经网络最终收敛何处？](<https://zhuanlan.zhihu.com/p/48737640>)



## QA

---

### 1. 如何避免陷入局部最小值与鞍点？

- SGD 或 Mini-batch：SGD 与 Mini-batch 引入了随机性，每次以部分样本来计算梯度，能够相当程度上避免陷入局部最小值。
- 动量： 引入动量，相当于引入惯性。一些常见情况时，如上次梯度过大，导致进入局部最小点时，下一次更新能很容易借助上次的大梯度跳出局部最小点。
- 自适应学习率：通过学习率来控制梯度是一个很棒的思想， 自适应学习率算法能够基于历史的累计梯度去计算一个当前较优的学习率。

