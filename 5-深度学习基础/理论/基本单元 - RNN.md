# RNN 

tags: 深度学习

---

[TOC]

## 基础相关

- [RNN ： NLP中最常见的神经网络单元](https://zhuanlan.zhihu.com/p/44106750)
- [LSTM：RNN最常用的变体](https://zhuanlan.zhihu.com/p/44124492)
- [RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)
- [RNN vs LSTM vs GRU -- 该选哪个？](https://zhuanlan.zhihu.com/p/55386469)

## QA

### 1. RNN 中为何会出现梯度消失，梯度爆炸问题

[RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)

因此， RNN 的梯度消失，梯度爆炸问题在于： 
```math
\prod_{j=k+1}^t \frac{\delta S_j}{\delta S_{j-1}} = \prod_{j=k+1}^t tanh' W_s
```

### 2. Relu 能否作为RNN的激活函数

答案是可以，但会产生一些问题：

> - 换成 Relu 可能使得输出值变得特别大，从而产生溢出
> - 换成Relu 也不能解决梯度消失，梯度爆炸问题，因为还有 $W_s$ 连乘的存在（如1中公式）

为什么 CNN 和前馈神经网络采用Relu 就能解决梯度消失，梯度爆炸问题？

> 因为CNN 或 FNN 中各层的 W 并不相同， 且初始化时是独立同分布的，一定程度熵可以抵消。
>
> 而 RNN 中各层矩阵 $W_s$ 是一样的。

### 3. 推导 LSTM

关键在于三个门， 三个状态。 其中三个门的公式基本一样

- 遗忘门： 决定上一时刻的 `$c_{t-1}$` 多少保留到当前时刻 `$c_t$`：

```math
f_t = \sigma{(W_f \cdot [h_{t-1},x_t] + b_f)} 
```

- 输入门：决定当前时刻输入 `$x_t$` 有多少保存到当前时刻 `$c_t$`：
  ```math
   i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i) 
  ```

- 输出门：控制当前时刻的 `$c_t$` 有多少信息作为当前时刻的 `$h_t$`:
  ```math
   o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o) 
  ```

- 当前输入的状态 `$\tilde{c_t}$`： 将上一时刻的 `$h_{t-1}$` 与当前时刻的 `$x_t$` 融合：
  ```math
  \tilde{c}_t=\tanh(W_c\cdot[h_{t-1},x_t]+b_c)
  ```

- 当前时刻的状态 `$c_t$`： 将 `$x_t$` ， `$\tilde{c_t}$` , `$h_{t-1}$` 融合：
  ```math
   c_t=f_t \circ c_{t-1}+i_t \circ \tilde{c}_t 
  ```

- 当前时刻的输出 $h_t$：从 $c_t$ 中分出一部分信息：
  ```math
  h_t=o_t\circ \tanh(c_t)
  ```




### 4. LSTM 长短记忆机制
0
长短记忆机制主要通过 **输入门** 与 **遗忘门** 来实现：

- 如果当前信息`$x_t$`不重要， 则**输入门**相应维度接近于 0， 当前的信息就几乎不融入进入 `$\tilde{c_t}$` 。反之， 输入门相应维度接近于 1， 当前信息实现很好的融入。
- 如果之前的信息 `$c_{t-1}$` 不重要，则遗忘门相应维度接近于 0， 过去的信息就几乎不融入进  `$c_t$`。 反之，亦然。

### 5. LSTM的门机制为何选择 sigmoid 作为激活函数？

值得一提的是， 目前几乎所有主流的门控机制中，门控单元的选择均使用 sigmoid 。

- sigmoid 的饱和性： 十分符合 门控 的效果
- 值域在 (0,1)， 当输入较大或较小时，输出会接近1 或 0， 从而保证门的开或关。

### 6.  融合信息时为何选择 tanh？

- 值域为 (-1, 1)， 这样会带来两个好处：

  > - 与大多数情景下特征分布以 0 为中心相吻合。（激活函数一章中有提到这点特性的重要性）
  > - 可以避免前向传播时的数值溢出问题(主要是上溢)

- tanh 在 0 附近有较大的梯度，模型收敛更快

### 7. 计算资源有限的情况下有没有什么优化方法？

- 采用 Hard gate
- 采用 GRU

### 8. 推导一下 GRU 

两个门，两个状态

- 更新门：控制前一时刻状态信息与当前输入融合
  ```math
  z_t = \sigma (W_z x_t + U_z h_{t-1}) 
  ```

- 重置门：控制前一时刻状态信息
  ```math
  r_t = \sigma(W_r x_t + U_r h_{t-1})
  ```

- 当前输入的信息融入 `$h_t'$`： 
  ```math
  h_t' = tanh(Wx_t + r_t \odot Uh_{t-1}) \\
  ```

- 当前时刻的状态：
  ```math
  h_t = z_t \odot h_{t-1} + (1-z_t) \odot h_{t-1}'
  ```




### 9. LSTM 与 GRU 之间的关系

- GRU 认为**遗忘门**与**输入门**功能有一定的的重合，认为之前的信息 与 当前的输入信息是此消彼长的关系，因此将二者合并成一个门： 更新门。
- 合并了记忆状态 c 与隐藏状态 h
- 采用**重置门**代替了**输出门**

### 10 为何RNN 会有梯度消失现象，推一下？

[RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)

### 11. LSTM 与 GRU 区别

- LSTM 中的单元状态 c 与 GRU 中的 h 类似，但 GRU 去掉了 h 这个状态，即最后的输出不再进行调节，那么也就不需要输出门了
- 在产生新的全局状态时， LSTM 采用 输入门+遗忘门 的方式， 而 GRU 只采用更新门来控制
- 更新门起到了遗忘门的作用， 重置门起到了输入门的作用。

### 12. 为何 RNN 训练时 loss 波动很大

由于RNN特有的memory会影响后期其他的RNN的特点，梯度时大时小，learning rate没法个性化的调整，导致RNN在train的过程中，Loss会震荡起伏。

为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。

### 13. LSTM 中的激活函数选择

LSTM， 门的激活函数选择 Sigmoid， 而在生成 c 时采用 tanh。

- Sigmoid函数的输出在0～1之间，符合门控的物理定义。且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关。
- 在生成候选记忆时，使用Tanh函数，是因为其输出在−1～1之间，这与大多数场景下特征分布是0中心的吻合。此外，Tanh函数在输入为0附近相比Sigmoid函数有更大的梯度，通常使模型收敛更快。