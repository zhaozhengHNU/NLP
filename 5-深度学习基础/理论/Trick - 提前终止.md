# Trick - 提前终止

tags: 深度学习

---

前期对该Trick的重视度不够， 想着，不就是过拟合吗？ 我只要保存 验证集 loss 最低点的模型， 你过拟合管我什么事情， 然而， 上了 Bert + 中大规模数据集之后， 我才发现什么叫做噩梦 -- 训练时间太 bb 长了吧。 

那还能咋地， 你总不能把 epoch 数量设置的太小吧， 10 是比较基本的吧（看数据集)，但这样跑也能跑个一天一夜，枯了。 这个时候这个Trick 就很有用了，大大节省模型训练时间。

提前终止指的是**当验证集的误差连续 `n`次递增时停止训练**。 一般情况下， 我会先将`n`设置为一个较大的数， 跑一个较大的 epoch（如20）， 然后根据 loss 曲线来选择合适的 epoch 数量和 `n`， 接下来就欢快的嘿嘿嘿了。

需要注意的是，提前终止有其他很多策略，上面提到的只是很常用的一种，至于什么时候选择什么样的提前终止策略也是一个经验活， 还好， [1] 中为我们提供了一些参考， 值得一看。 由于本人经验时间优先，没有深入探讨，希望有大佬帮忙解惑。下面我简单提一下[1] 中是如何建议的。

## Early Stop, but when? [1]

首先，思考一个问题： 如果我们以验证集loss最小值来作为保存模型的依据，那么这会不会对模型的泛化能力产生影响？我个人认为是**不会**，具体需要看Loss曲线图。一般而言，第一次跑的时候，我往往会设置一个很大的 epoch， 这样能够直观的从Loss曲线上看到很多信息，从而帮助我来选择更佳合适 epoch数量， 我个人一般取开始过拟合之后两个epoch作为epoch的最终数量。

我自己做实验发现，在验证集loss最低点不一定能够获得在测试集上最好的效果，有时候，最低点附近的一些点反而在测试集上表现更佳，但差别不大，这也是我认为不会影响模型泛化能力的一个依据， 而实际落地中，这点差别其实真的没有啥鸟用。

回到Trick， 文章谈到， 何时提前停止是对训练时间和泛化误差之间的权衡， 而如果我采用在验证集loss最低点存储模型的方式，那么何时提前停止就不再是对泛化误差的权衡了， 其变成了，如何选择何种的策略来对**训练时间**以及**获取验证集loss全局最低点**来做权衡，使得我们能够找到该点并保存模型，而不至于陷入一些困境[2]。

最后谈谈 Early Stop 策略， 选择合适的策略能够使得我们在合适的时间终止整个训练，从而节省时间。

