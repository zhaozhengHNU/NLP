# 正则化

---

https://blog.csdn.net/jinping_shi/article/details/52433975

https://zhuanlan.zhihu.com/p/38709373

https://blog.csdn.net/heyongluoyao8/article/details/49429629

## 1. L1 正则化 - 稀疏正则化

1-范数: 表示向量元素的绝对值之和。
```math
||x|| =\sum_{i=1}^N |x_i|
```

```math
正则化项： \Omega(\theta) = ||w||_1 =  \sum_i |w_i| \\
目标函数： \tilde{J}(w;X,y) = \alpha ||w||_1  + J(w;X,y)  \\
梯度： \nabla_w \tilde{J}(w;X,y) = \alpha sign(w) + \nabla_w J(w;X,y) \\
```

不同于L2，L1 正则化使得权重值可能被减少到0。 因此，L1对于压缩模型很有用。

稀疏向量通常会有许多维度，如果再加上使用特征组合会导致包含更多的维度的。由于使用此类高维度特征向量，因此模型可能会非常庞大，并且需要大量的 RAM。

在高维度稀疏矢量中，最好尽可能使权重正好降至 `0`。正好为 0 的权重基本上会使相应特征从模型中移除。 将特征设为 0 可节省 RAM 空间，且可以减少模型中的噪点。

## 2. L2 正则化 -- 权重衰减

2-范数： 表示向量元素绝对值的平方和再开方。
```math
||x|| = \sqrt{\sum_{i=1}^N x_i^2}
```

```math
正则化项： \Omega(\theta) = \frac{1}{2} ||w||_2^2  = \frac{1}{2}w^Tw \\
目标函数： \tilde{J}(w;X,y) = \frac{\alpha}{2}w^Tw  + J(w;X,y)  \\
梯度： \nabla_w \tilde{J}(w;X,y) = \alpha w + \nabla_w J(w;X,y) \\
梯度更新 ： w \leftarrow (1- \epsilon \alpha) w - \epsilon \nabla_w J(w;X,y)
```

L2正则化又称权重衰减。因为其导致权重**趋向于0**（但不全是0）。

执行 L2 正则化对模型具有以下影响：

- 使权重值接近于 0（但并非正好为 0）
- 使权重的平均值接近于 0，且呈正态分布。



---

## QA

### 1. 为何只对权重进行正则惩罚，而不针对偏置

在神经网络中，参数包括每一层仿射变换的**权重**和**偏置**，我们通常只对权重做惩罚而不对偏置做正则惩罚。

精确拟合偏置所需的数据通常比拟合权重少得多。每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的欠拟合。

### 2. 权重衰减的目的

限制模型的学习能力，通过限制参数 θ 的规模（主要是权重 w 的规模，偏置 b 不参与惩罚），使模型偏好于权值较小的目标函数，防止过拟合。

### 3. L1 与 L2 的异同

- 相同点：限制模型的学习能力，通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。

- 不同点：

  > - L1是模型各个参数的绝对值之和；L2为各个参数平方和的开方值。
  > - L1 正则化可以产生**稀疏权值矩阵**，即产生一个稀疏模型，可以用于特征选择；L2 会趋向于生成一个参数值很小的矩阵。
  > - L1 适用于特征之间有关联的情况； L2 适用于特征之间没有关联的情况

### 4.为什么 L1 正则化 可以产生稀疏值，而 L2 不会？

添加 L1 正则化，相当于在 L1范数的约束下求目标函数 J 的最小值，下图展示了二维的情况：

![L1](..\img\正则化\L1.png)


图中 J 与 L 首次相交的点就是最优解。L1 在和每个坐标轴相交的地方都会有“角”出现（多维的情况下，这些角会更多），在角的位置就会产生稀疏的解。而 J 与这些“角”相交的机会远大于其他点，因此 L1 正则化会产生稀疏的权值。

对于 L2 正则化来说，其二维平面下的图形为：

![L2](..\img\正则化\L2.png)

如上图所示， 相比于 L1 正则化， L2 不会产生 “角”， 因此 J 与 L2 相交的点具有稀疏性的概率就会变得非常小。

### 5. 为何 L1 和 L2 正则化可以防止过拟合？

L1 & L2 正则化会使模型偏好于更小的权值。

简单来说，更小的权值意味着更低的模型复杂度，也就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如异常点，噪声），以提高模型的泛化能力。

此外，添加正则化相当于为模型添加了某种限制，规定了参数的分布，从而降低了模型的复杂度。模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。 -- **奥卡姆剃刀原理**

